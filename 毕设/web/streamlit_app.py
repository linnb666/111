import streamlit as st
import sys
from pathlib import Path
import cv2
import tempfile
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from config.config import STREAMLIT_CONFIG, POSE_CONFIG, VIEW_DETECTION_CONFIG
from modules.video_processor import VideoProcessor
from modules.pose_estimator import create_pose_estimator
from modules.kinematic_analyzer import KinematicAnalyzer
from modules.temporal_model import TemporalModelAnalyzer
from modules.quality_evaluator import QualityEvaluator
from modules.ai_analyzer import AIAnalyzer
from modules.database import DatabaseManager
from modules.view_detector import ViewAngleDetector, AdaptiveAnalyzer

# é¡µé¢é…ç½®
st.set_page_config(
    page_title=STREAMLIT_CONFIG['page_title'],
    page_icon=STREAMLIT_CONFIG['page_icon'],
    layout=STREAMLIT_CONFIG['layout']
)


# åˆå§‹åŒ–ç»„ä»¶
@st.cache_resource
def init_components():
    """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
    return {
        'db': DatabaseManager(),
        'ai': AIAnalyzer()
    }


components = init_components()


def main():
    """ä¸»ç•Œé¢"""
    st.title("ğŸƒ è·‘æ­¥åŠ¨ä½œåˆ†æç³»ç»Ÿ")
    st.markdown("*åŸºäºæ·±åº¦å­¦ä¹ çš„è·‘æ­¥åŠ¨ä½œè§†é¢‘è§£æä¸æŠ€æœ¯è´¨é‡è¯„ä»·*")
    st.markdown("---")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ“‹ å¯¼èˆª")
        page = st.radio(
            "é€‰æ‹©åŠŸèƒ½",
            ["è§†é¢‘åˆ†æ", "å†å²è®°å½•", "ç³»ç»Ÿç»Ÿè®¡", "ç³»ç»Ÿè®¾ç½®"]
        )

        st.markdown("---")
        st.info("ğŸ’¡ ä¸Šä¼ è·‘æ­¥è§†é¢‘ï¼Œè·å–ä¸“ä¸šæŠ€æœ¯åˆ†æ")

        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        st.markdown("---")
        st.caption("ç³»ç»Ÿä¿¡æ¯")
        st.caption(f"å§¿æ€ä¼°è®¡: {POSE_CONFIG['backend'].upper()}")

    # ä¸»å†…å®¹åŒº
    if page == "è§†é¢‘åˆ†æ":
        video_analysis_page()
    elif page == "å†å²è®°å½•":
        history_page()
    elif page == "ç³»ç»Ÿç»Ÿè®¡":
        statistics_page()
    elif page == "ç³»ç»Ÿè®¾ç½®":
        settings_page()


def video_analysis_page():
    """è§†é¢‘åˆ†æé¡µé¢"""
    st.header("ğŸ“¹ è§†é¢‘åˆ†æ")

    # åˆ†æé€‰é¡¹
    with st.expander("âš™ï¸ åˆ†æé€‰é¡¹", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            auto_detect_view = st.checkbox("è‡ªåŠ¨æ£€æµ‹è§†è§’", value=True,
                                           help="è‡ªåŠ¨è¯†åˆ«è§†é¢‘æ˜¯ä¾§é¢ã€æ­£é¢è¿˜æ˜¯æ··åˆè§†è§’")
        with col2:
            manual_view = st.selectbox(
                "æ‰‹åŠ¨æŒ‡å®šè§†è§’",
                ["è‡ªåŠ¨", "ä¾§é¢", "æ­£é¢", "èƒŒé¢"],
                disabled=auto_detect_view
            )

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ è·‘æ­¥è§†é¢‘",
        type=['mp4', 'avi', 'mov', 'mkv'],
        help="æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼ï¼Œå»ºè®®ä½¿ç”¨ä¾§é¢æˆ–æ­£é¢æ‹æ‘„çš„è§†é¢‘"
    )

    if uploaded_file is not None:
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
            tmp_file.write(uploaded_file.read())
            video_path = tmp_file.name

        # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
        st.video(uploaded_file)

        # åˆ†ææŒ‰é’®
        if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary"):
            view_override = None if auto_detect_view else {
                "è‡ªåŠ¨": None, "ä¾§é¢": "side", "æ­£é¢": "front", "èƒŒé¢": "back"
            }.get(manual_view)
            analyze_video(video_path, view_override)


def analyze_video(video_path: str, view_override: str = None):
    """æ‰§è¡Œè§†é¢‘åˆ†æ"""
    try:
        # è¿›åº¦æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        # 1. è§†é¢‘é¢„å¤„ç†
        status_text.text("1ï¸âƒ£ è§†é¢‘é¢„å¤„ç†ä¸­...")
        progress_bar.progress(5)
        processor = VideoProcessor(video_path)
        video_info = processor.get_video_info()
        frames, fps = processor.extract_frames(target_fps=30, max_frames=300)

        # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("åˆ†è¾¨ç‡", f"{video_info['width']}x{video_info['height']}")
        col2.metric("å¸§ç‡", f"{video_info['fps']:.1f} FPS")
        col3.metric("æ—¶é•¿", f"{video_info['duration']:.1f} ç§’")
        col4.metric("æå–å¸§æ•°", f"{len(frames)}")

        # 2. å§¿æ€ä¼°è®¡
        status_text.text("2ï¸âƒ£ å§¿æ€ä¼°è®¡ä¸­...")
        progress_bar.progress(20)
        estimator = create_pose_estimator(POSE_CONFIG['backend'], POSE_CONFIG)
        keypoints_sequence = estimator.process_frames(frames)

        detected_count = sum(1 for kp in keypoints_sequence if kp['detected'])
        st.info(f"âœ“ å§¿æ€æ£€æµ‹æˆåŠŸ: {detected_count}/{len(keypoints_sequence)} å¸§ ({detected_count/len(keypoints_sequence)*100:.1f}%)")

        # 3. è§†è§’æ£€æµ‹
        status_text.text("3ï¸âƒ£ è§†è§’æ£€æµ‹ä¸­...")
        progress_bar.progress(30)

        if view_override:
            detected_view = view_override
            view_confidence = 1.0
            st.info(f"ğŸ“ ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šè§†è§’: {get_view_name(detected_view)}")
        else:
            view_detector = ViewAngleDetector()
            view_result = view_detector.detect_view_angle(keypoints_sequence)
            detected_view = view_result['primary_view']  # ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„é”®å
            view_confidence = view_result['confidence']

            # æ˜¾ç¤ºè§†è§’æ£€æµ‹ç»“æœ
            view_col1, view_col2, view_col3 = st.columns(3)
            view_col1.metric("æ£€æµ‹è§†è§’", get_view_name(detected_view))
            view_col2.metric("ç½®ä¿¡åº¦", f"{view_confidence*100:.1f}%")
            view_col3.metric("åˆ†æç­–ç•¥", get_strategy_name(detected_view))

        # ç”Ÿæˆå§¿æ€è¯†åˆ«å†…å®¹
        status_text.text("3ï¸âƒ£ ç”Ÿæˆå§¿æ€è¯†åˆ«è§†é¢‘ä¸å…³é”®å¸§...")
        progress_bar.progress(40)

        # å°è¯•ç”Ÿæˆè§†é¢‘
        try:
            pose_video_path = generate_pose_video(frames, keypoints_sequence, fps, estimator)
            st.subheader("ğŸ¦´ å§¿æ€è¯†åˆ«è§†é¢‘")
            st.video(pose_video_path)
        except Exception as video_err:
            st.warning(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {video_err}ï¼Œå°†æ˜¾ç¤ºå…³é”®å¸§å›¾åƒ")

        # æå–å¹¶æ˜¾ç¤ºå…³é”®å¸§ï¼ˆæ— è®ºè§†é¢‘æ˜¯å¦æˆåŠŸéƒ½æ˜¾ç¤ºï¼‰
        keyframe_data = extract_keyframes_with_poses(frames, keypoints_sequence, fps, estimator, num_keyframes=6)
        if keyframe_data:
            st.subheader("ğŸ–¼ï¸ å…³é”®å¸§å§¿æ€åˆ†æ")

            # æ¯è¡Œæ˜¾ç¤º3å¼ å…³é”®å¸§
            for row_start in range(0, len(keyframe_data), 3):
                cols = st.columns(3)
                for i, kf in enumerate(keyframe_data[row_start:row_start+3]):
                    with cols[i]:
                        st.image(kf['path'], caption=f"æ—¶é—´: {kf['time_sec']:.2f}s",
                                 use_container_width=True)
                        if not kf['detected']:
                            st.caption("âš ï¸ æœªæ£€æµ‹åˆ°å§¿æ€")

        # 4. è¿åŠ¨å­¦åˆ†æï¼ˆä½¿ç”¨è‡ªé€‚åº”åˆ†æå™¨ï¼‰
        status_text.text("4ï¸âƒ£ è¿åŠ¨å­¦åˆ†æä¸­...")
        progress_bar.progress(55)

        adaptive_analyzer = AdaptiveAnalyzer()
        kinematic_results = adaptive_analyzer.analyze(
            keypoints_sequence, fps,
            view_angle=detected_view if not view_override else view_override
        )

        # 5. æ·±åº¦å­¦ä¹ åˆ†æ
        status_text.text("5ï¸âƒ£ æ·±åº¦å­¦ä¹ åˆ†æä¸­...")
        progress_bar.progress(70)
        temporal_analyzer = TemporalModelAnalyzer()
        temporal_results = temporal_analyzer.analyze(keypoints_sequence)

        # 6. è´¨é‡è¯„ä»·
        status_text.text("6ï¸âƒ£ æŠ€æœ¯è´¨é‡è¯„ä»·ä¸­...")
        progress_bar.progress(85)
        quality_evaluator = QualityEvaluator()
        quality_results = quality_evaluator.evaluate(
            kinematic_results, temporal_results,
            view_angle=detected_view
        )

        # 7. AIæ–‡æœ¬ç”Ÿæˆ
        status_text.text("7ï¸âƒ£ AIæ–‡æœ¬åˆ†æä¸­...")
        progress_bar.progress(90)
        results_for_ai = {
            'quality_evaluation': quality_results,
            'kinematic_analysis': kinematic_results,
            'temporal_analysis': temporal_results,
            'view_angle': detected_view
        }
        ai_text = components['ai'].generate_analysis_report(results_for_ai)

        # 8. å¤šæ¨¡æ€æ—¶é—´æ®µåˆ†æï¼ˆå¦‚æœæœ‰å…³é”®å¸§æ•°æ®ï¼‰
        time_segment_analysis = ""
        if keyframe_data and len(keyframe_data) > 0:
            status_text.text("8ï¸âƒ£ å¤šæ¨¡æ€æ—¶é—´æ®µåˆ†æä¸­...")
            progress_bar.progress(95)
            try:
                time_segment_analysis = components['ai'].analyze_time_segments(
                    keyframe_data, kinematic_results
                )
            except Exception as e:
                st.warning(f"æ—¶é—´æ®µåˆ†æå¤±è´¥: {e}")
                time_segment_analysis = ""

        # å®Œæˆ
        progress_bar.progress(100)
        status_text.text("âœ… åˆ†æå®Œæˆ!")

        # æ˜¾ç¤ºç»“æœ
        st.markdown("---")
        display_results(quality_results, kinematic_results, temporal_results, ai_text, detected_view, time_segment_analysis)

        # ä¿å­˜åˆ°æ•°æ®åº“
        complete_results = {
            'video_info': video_info,
            'kinematic_analysis': kinematic_results,
            'temporal_analysis': temporal_results,
            'quality_evaluation': quality_results,
            'ai_analysis': ai_text,
            'view_angle': detected_view
        }
        record_id = components['db'].save_analysis(complete_results)
        st.success(f"åˆ†æç»“æœå·²ä¿å­˜ (ID: {record_id})")

        # æ¸…ç†èµ„æº
        processor.release()
        estimator.close()

    except Exception as e:
        st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        st.code(traceback.format_exc())


def get_view_name(view: str) -> str:
    """è·å–è§†è§’ä¸­æ–‡åç§°"""
    names = {
        'side': 'ä¾§é¢è§†è§’',
        'front': 'æ­£é¢è§†è§’',
        'back': 'èƒŒé¢è§†è§’',
        'mixed': 'æ··åˆè§†è§’'
    }
    return names.get(view, view)


def get_strategy_name(view: str) -> str:
    """è·å–åˆ†æç­–ç•¥åç§°"""
    strategies = {
        'side': 'è†è§’+æŒ¯å¹…+èº¯å¹²',
        'front': 'å¯¹ç§°æ€§+é«‹éƒ¨+è†å¤–ç¿»',
        'back': 'å¯¹ç§°æ€§+è¶³è·Ÿ',
        'mixed': 'ç»¼åˆåˆ†æ'
    }
    return strategies.get(view, 'æ ‡å‡†åˆ†æ')


def generate_pose_video(frames, keypoints_sequence, fps, estimator):
    """å°†å§¿æ€éª¨æ¶ç»˜åˆ¶åˆ°æ¯ä¸€å¸§å¹¶ç”Ÿæˆè§†é¢‘"""
    import tempfile
    import os

    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…è·¯å¾„é—®é¢˜
    output_dir = Path("output/videos")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨å”¯ä¸€çš„æ–‡ä»¶å
    import time
    timestamp = int(time.time())
    output_path = output_dir / f"pose_visualization_{timestamp}.mp4"

    h, w = frames[0].shape[:2]
    fps_int = max(1, int(round(fps)))

    # å°è¯•å¤šç§ç¼–ç æ ¼å¼
    codecs = [
        ('avc1', '.mp4'),  # H.264 - æœ€å…¼å®¹
        ('mp4v', '.mp4'),  # MPEG-4
        ('XVID', '.avi'),  # XVID
    ]

    writer = None
    final_path = None

    for codec, ext in codecs:
        test_path = output_dir / f"pose_visualization_{timestamp}{ext}"
        fourcc = cv2.VideoWriter_fourcc(*codec)
        writer = cv2.VideoWriter(
            str(test_path),
            fourcc,
            fps_int,
            (w, h)
        )
        if writer.isOpened():
            final_path = test_path
            break
        writer.release()

    if not writer or not writer.isOpened():
        raise RuntimeError("âŒ VideoWriter æ‰“å¼€å¤±è´¥ï¼Œå°è¯•äº†å¤šç§ç¼–ç æ ¼å¼")

    for frame, kp in zip(frames, keypoints_sequence):
        if kp.get("detected", False):
            vis_frame = estimator.visualize_pose(frame, kp)
        else:
            vis_frame = frame.copy()

        writer.write(vis_frame)

    writer.release()

    return str(final_path)


def extract_keyframes_with_poses(frames, keypoints_sequence, fps, estimator, num_keyframes=6):
    """æå–å…³é”®å¸§å¹¶ç»˜åˆ¶å§¿æ€éª¨æ¶"""
    import time

    output_dir = Path("output/keyframes")
    output_dir.mkdir(parents=True, exist_ok=True)

    total_frames = len(frames)
    if total_frames == 0:
        return []

    # è®¡ç®—å…³é”®å¸§ç´¢å¼•ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
    if total_frames <= num_keyframes:
        indices = list(range(total_frames))
    else:
        indices = [int(i * (total_frames - 1) / (num_keyframes - 1)) for i in range(num_keyframes)]

    keyframe_paths = []
    timestamp = int(time.time())

    for i, idx in enumerate(indices):
        frame = frames[idx]
        kp = keypoints_sequence[idx]

        if kp.get("detected", False):
            vis_frame = estimator.visualize_pose(frame.copy(), kp)
        else:
            vis_frame = frame.copy()
            # åœ¨æœªæ£€æµ‹åˆ°å§¿æ€çš„å¸§ä¸Šæ·»åŠ æç¤º
            cv2.putText(vis_frame, "No pose detected", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # æ·»åŠ æ—¶é—´æˆ³
        time_sec = idx / fps
        cv2.putText(vis_frame, f"Time: {time_sec:.2f}s", (10, vis_frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # ä¿å­˜å…³é”®å¸§
        keyframe_path = output_dir / f"keyframe_{timestamp}_{i}.jpg"
        cv2.imwrite(str(keyframe_path), vis_frame)
        keyframe_paths.append({
            'path': str(keyframe_path),
            'frame_idx': idx,
            'time_sec': time_sec,
            'detected': kp.get("detected", False)
        })

    return keyframe_paths


def display_results(quality, kinematic, temporal, ai_text, view_angle='side', time_segment_analysis=''):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    st.header("ğŸ“Š åˆ†æç»“æœ")

    # æ€»ä½“è¯„åˆ†
    st.subheader("ğŸ¯ æ€»ä½“è¯„ä»·")
    col1, col2, col3 = st.columns([1, 1, 2])

    with col1:
        score = quality['total_score']
        st.metric("æŠ€æœ¯è´¨é‡è¯„åˆ†", f"{score:.1f}/100",
                  delta=f"{quality['rating']}")

    with col2:
        st.metric("åˆ†æè§†è§’", get_view_name(view_angle))

    with col3:
        st.markdown(f"**è¯„çº§:** {quality['rating']}")
        if quality.get('strengths'):
            st.markdown(f"**ä¼˜åŠ¿:** {', '.join(quality['strengths'][:3])}")
        if quality.get('weaknesses'):
            st.markdown(f"**è–„å¼±é¡¹:** {', '.join(quality['weaknesses'][:3])}")

    # å„ç»´åº¦å¾—åˆ†
    st.subheader("ğŸ“ˆ å„ç»´åº¦è¡¨ç°")
    cols = st.columns(4)
    dimensions = quality.get('dimension_scores', {})

    cols[0].metric("ç¨³å®šæ€§", f"{dimensions.get('stability', 0):.1f}")
    cols[1].metric("æ•ˆç‡", f"{dimensions.get('efficiency', 0):.1f}")
    cols[2].metric("è·‘å§¿", f"{dimensions.get('form', 0):.1f}")
    cols[3].metric("èŠ‚å¥", f"{dimensions.get('rhythm', 0):.1f}")

    # è¿åŠ¨å­¦æŒ‡æ ‡ - æ ¹æ®è§†è§’æ˜¾ç¤ºä¸åŒä¿¡æ¯
    st.subheader("ğŸ”¬ è¿åŠ¨å­¦æŒ‡æ ‡")

    # åŸºç¡€æŒ‡æ ‡ï¼ˆæ‰€æœ‰è§†è§’éƒ½æ˜¾ç¤ºï¼‰
    cadence_data = kinematic.get('cadence', {})
    col1, col2, col3 = st.columns(3)
    col1.metric("æ­¥é¢‘", f"{cadence_data.get('cadence', 0):.1f} æ­¥/åˆ†",
                delta=cadence_data.get('rating', {}).get('description', ''))
    col2.metric("æ£€æµ‹æ­¥æ•°", f"{cadence_data.get('step_count', 0)} æ­¥",
                help=f"è§†é¢‘æ—¶é•¿ {cadence_data.get('duration', 0):.1f} ç§’")

    # å‚ç›´æŒ¯å¹… - ä½¿ç”¨å½’ä¸€åŒ–å€¼
    vertical_motion = kinematic.get('vertical_motion', {})
    if 'amplitude_normalized' in vertical_motion:
        amplitude_pct = vertical_motion['amplitude_normalized']
        rating_info = vertical_motion.get('amplitude_rating', {})
        col3.metric("å‚ç›´æŒ¯å¹…", f"{amplitude_pct:.1f}% èº¯å¹²",
                    delta=rating_info.get('description', ''),
                    help="ç›¸å¯¹äºèº¯å¹²é•¿åº¦çš„å‚ç›´æŒ¯å¹…ç™¾åˆ†æ¯”")
    elif vertical_motion.get('amplitude', 0) > 0:
        col3.metric("å‚ç›´æŒ¯å¹…", f"{vertical_motion['amplitude']:.4f}",
                    help="å½’ä¸€åŒ–åæ ‡ä¸‹çš„æŒ¯å¹…")
    else:
        col3.metric("å‚ç›´æŒ¯å¹…", "æ•°æ®ä¸è¶³")

    # è†å…³èŠ‚è§’åº¦åˆ†æï¼ˆä¾§é¢è§†è§’é‡ç‚¹ï¼‰
    if view_angle in ['side', 'mixed']:
        angles = kinematic.get('angles', {})

        # phase_analysis ç›´æ¥åœ¨ angles ä¸‹ï¼Œä¸æ˜¯åœ¨ angles['knee'] ä¸‹
        if 'phase_analysis' in angles:
            st.subheader("ğŸ¦µ è†å…³èŠ‚è§’åº¦åˆ†æï¼ˆåˆ†é˜¶æ®µï¼‰")
            phase_analysis = angles['phase_analysis']

            phase_cols = st.columns(3)

            # è§¦åœ°é˜¶æ®µ
            gc = phase_analysis.get('ground_contact', {})
            with phase_cols[0]:
                st.markdown("**è§¦åœ°é˜¶æ®µ**")
                st.metric("å¹³å‡è§’åº¦", f"{gc.get('mean', 0):.1f}Â°")
                st.caption(f"èŒƒå›´: {gc.get('min', 0):.1f}Â° - {gc.get('max', 0):.1f}Â°")
                st.caption(f"å¸§æ•°: {gc.get('count', 0)}")

            # è…¾ç©ºé˜¶æ®µ
            fl = phase_analysis.get('flight', {})
            with phase_cols[1]:
                st.markdown("**è…¾ç©ºé˜¶æ®µ**")
                st.metric("å¹³å‡è§’åº¦", f"{fl.get('mean', 0):.1f}Â°")
                st.caption(f"èŒƒå›´: {fl.get('min', 0):.1f}Â° - {fl.get('max', 0):.1f}Â°")
                st.caption(f"å¸§æ•°: {fl.get('count', 0)}")

            # è¿‡æ¸¡é˜¶æ®µ
            tr = phase_analysis.get('transition', {})
            with phase_cols[2]:
                st.markdown("**è¿‡æ¸¡é˜¶æ®µ**")
                st.metric("å¹³å‡è§’åº¦", f"{tr.get('mean', 0):.1f}Â°")
                st.caption(f"èŒƒå›´: {tr.get('min', 0):.1f}Â° - {tr.get('max', 0):.1f}Â°")
                st.caption(f"å¸§æ•°: {tr.get('count', 0)}")

    # å¯¹ç§°æ€§åˆ†æï¼ˆæ­£é¢/èƒŒé¢è§†è§’é‡ç‚¹ï¼‰
    if view_angle in ['front', 'back', 'mixed']:
        symmetry = kinematic.get('symmetry', {})
        if symmetry:
            st.subheader("âš–ï¸ å¯¹ç§°æ€§åˆ†æ")
            sym_cols = st.columns(3)

            sym_cols[0].metric("è‚©éƒ¨å¯¹ç§°æ€§",
                               f"{symmetry.get('shoulder_symmetry', 0)*100:.1f}%")
            sym_cols[1].metric("é«‹éƒ¨å¯¹ç§°æ€§",
                               f"{symmetry.get('hip_symmetry', 0)*100:.1f}%")
            sym_cols[2].metric("æ•´ä½“å¯¹ç§°æ€§",
                               f"{symmetry.get('overall_symmetry', 0)*100:.1f}%")

    # æ·±åº¦å­¦ä¹ ç»“æœ
    st.subheader("ğŸ¤– æ·±åº¦å­¦ä¹ åˆ†æ")
    col1, col2 = st.columns(2)

    col1.metric("AIè´¨é‡è¯„åˆ†", f"{temporal.get('quality_score', 0):.1f}")
    col2.metric("AIç¨³å®šæ€§", f"{temporal.get('stability_score', 0):.1f}")

    phase_dist = temporal.get('phase_distribution', {})
    if phase_dist:
        st.markdown(f"**é˜¶æ®µåˆ†å¸ƒ:** è§¦åœ° {phase_dist.get('ground_contact', 0) * 100:.1f}% | "
                    f"è…¾ç©º {phase_dist.get('flight', 0) * 100:.1f}% | "
                    f"è¿‡æ¸¡ {phase_dist.get('transition', 0) * 100:.1f}%")

    # æ”¹è¿›å»ºè®®
    if quality.get('suggestions'):
        st.subheader("ğŸ’¡ æ”¹è¿›å»ºè®®")
        for i, suggestion in enumerate(quality['suggestions'], 1):
            st.markdown(f"{i}. {suggestion}")

    # AIåˆ†ææ–‡æœ¬
    st.subheader("ğŸ“ AIæ·±åº¦åˆ†æ")
    st.markdown(ai_text)

    # æ—¶é—´æ®µé—®é¢˜åˆ†æï¼ˆå¤šæ¨¡æ€ï¼‰
    if time_segment_analysis:
        st.subheader("ğŸ” å¤šæ¨¡æ€æ—¶é—´æ®µåˆ†æ")
        st.markdown(time_segment_analysis)


def history_page():
    """å†å²è®°å½•é¡µé¢"""
    st.header("ğŸ“œ å†å²è®°å½•")

    records = components['db'].get_recent_analyses(20)

    if not records:
        st.info("æš‚æ— å†å²è®°å½•")
        return

    for record in records:
        with st.expander(f"ğŸ“¹ {record['video_filename']} - {record['analysis_date']}"):
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("è¯„åˆ†", f"{record['total_score']:.1f}")
            col2.metric("è¯„çº§", record['rating'])
            col3.metric("æ—¶é•¿", f"{record['video_duration']:.1f}ç§’")
            col4.metric("æ­¥é¢‘", f"{record['cadence']:.1f}")


def statistics_page():
    """ç»Ÿè®¡é¡µé¢"""
    st.header("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡")

    stats = components['db'].get_statistics()

    col1, col2 = st.columns(2)
    col1.metric("æ€»åˆ†ææ¬¡æ•°", stats['total_analyses'])
    col2.metric("å¹³å‡è¯„åˆ†", f"{stats['average_score']:.1f}")

    st.subheader("è¯„çº§åˆ†å¸ƒ")
    for rating, count in stats['rating_distribution'].items():
        st.markdown(f"**{rating}:** {count} æ¬¡")


def settings_page():
    """ç³»ç»Ÿè®¾ç½®é¡µé¢"""
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

    st.subheader("å§¿æ€ä¼°è®¡è®¾ç½®")
    st.info(f"å½“å‰åç«¯: {POSE_CONFIG['backend'].upper()}")
    st.caption("å¦‚éœ€åˆ‡æ¢å§¿æ€ä¼°è®¡åç«¯ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ config/config.py")

    st.subheader("è§†è§’æ£€æµ‹è®¾ç½®")
    with st.expander("æŸ¥çœ‹å½“å‰é…ç½®"):
        st.json(VIEW_DETECTION_CONFIG)

    st.subheader("AIåˆ†æè®¾ç½®")
    st.caption("æ”¯æŒçš„AIæä¾›å•†: OpenAI, Anthropic, é€šä¹‰åƒé—®, æ™ºè°±AI")
    st.caption("å¦‚éœ€å¯ç”¨AIåˆ†æï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®ç›¸åº”çš„APIå¯†é’¥")

    # æ˜¾ç¤ºç¯å¢ƒå˜é‡çŠ¶æ€
    import os
    providers = {
        'OpenAI': 'OPENAI_API_KEY',
        'Anthropic': 'ANTHROPIC_API_KEY',
        'é€šä¹‰åƒé—®': 'DASHSCOPE_API_KEY',
        'æ™ºè°±AI': 'ZHIPU_API_KEY'
    }

    st.markdown("**APIå¯†é’¥çŠ¶æ€:**")
    for name, env_var in providers.items():
        status = "âœ… å·²é…ç½®" if os.getenv(env_var) else "âŒ æœªé…ç½®"
        st.markdown(f"- {name}: {status}")


if __name__ == '__main__':
    main()
