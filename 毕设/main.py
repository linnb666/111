import sys
import argparse
from pathlib import Path

from config.config import OUTPUT_DIR, POSE_CONFIG
from modules.video_processor import VideoProcessor
from modules.pose_estimator import create_pose_estimator
from modules.kinematic_analyzer import KinematicAnalyzer
from modules.temporal_model import TemporalModelAnalyzer
from modules.quality_evaluator import QualityEvaluator
from modules.ai_analyzer import AIAnalyzer
from modules.database import DatabaseManager
from modules.view_detector import ViewAngleDetector, AdaptiveAnalyzer
from utils.visualization import create_comparison_video, plot_angle_curves


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è·‘æ­¥åŠ¨ä½œåˆ†æç³»ç»Ÿ')
    parser.add_argument('video_path', type=str, help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–ç»“æœ')
    parser.add_argument('--save-db', action='store_true', help='ä¿å­˜åˆ°æ•°æ®åº“')
    parser.add_argument('--view', type=str, choices=['auto', 'side', 'front', 'back'],
                        default='auto', help='è§†é¢‘è§†è§’ (auto=è‡ªåŠ¨æ£€æµ‹)')

    args = parser.parse_args()

    # éªŒè¯è§†é¢‘æ–‡ä»¶
    video_path = Path(args.video_path)
    if not video_path.exists():
        print(f"é”™è¯¯: è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {video_path}")
        sys.exit(1)

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output) if args.output else OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("åŸºäºæ·±åº¦å­¦ä¹ çš„è·‘æ­¥åŠ¨ä½œè§†é¢‘è§£æä¸æŠ€æœ¯è´¨é‡è¯„ä»·ç³»ç»Ÿ")
    print("=" * 80)
    print(f"è§†é¢‘æ–‡ä»¶: {video_path.name}")
    print(f"å§¿æ€ä¼°è®¡åç«¯: {POSE_CONFIG['backend'].upper()}")
    print(f"è§†è§’æ¨¡å¼: {args.view}")
    print("=" * 80)

    try:
        # æ‰§è¡Œåˆ†æ
        results = run_analysis_pipeline(
            str(video_path), output_dir, args.visualize,
            view_mode=args.view
        )

        # æ‰“å°ç»“æœ
        print_results(results)

        # ä¿å­˜åˆ°æ•°æ®åº“
        if args.save_db:
            db = DatabaseManager()
            record_id = db.save_analysis(results)
            print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ (ID: {record_id})")

        print("\n" + "=" * 80)
        print("âœ… åˆ†æå®Œæˆ!")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def run_analysis_pipeline(video_path: str, output_dir: Path, visualize: bool = False,
                          view_mode: str = 'auto'):
    """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""

    # 1. è§†é¢‘é¢„å¤„ç†
    print("\n1ï¸âƒ£ è§†é¢‘è¾“å…¥ä¸é¢„å¤„ç†...")
    processor = VideoProcessor(video_path)
    video_info = processor.get_video_info()
    print(f"   åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}")
    print(f"   å¸§ç‡: {video_info['fps']:.2f} FPS")
    print(f"   æ—¶é•¿: {video_info['duration']:.2f} ç§’")

    frames, fps = processor.extract_frames(target_fps=30, max_frames=300)
    print(f"   æå–å¸§æ•°: {len(frames)}")

    # 2. å§¿æ€ä¼°è®¡
    print("\n2ï¸âƒ£ äººä½“å§¿æ€ä¼°è®¡...")
    estimator = create_pose_estimator(POSE_CONFIG['backend'], POSE_CONFIG)
    keypoints_sequence = estimator.process_frames(frames)

    detected_count = sum(1 for kp in keypoints_sequence if kp['detected'])
    print(f"   æ£€æµ‹æˆåŠŸ: {detected_count}/{len(keypoints_sequence)} å¸§ ({detected_count/len(keypoints_sequence)*100:.1f}%)")

    # å¯è§†åŒ–å§¿æ€
    if visualize and detected_count > 0:
        print("   ç”Ÿæˆå§¿æ€å¯è§†åŒ–...")
        pose_frames = []
        for i, kp in enumerate(keypoints_sequence[:10]):  # ä»…å‰10å¸§
            pose_frame = estimator.visualize_pose(frames[i], kp)
            pose_frames.append(pose_frame)

        # ä¿å­˜ç¬¬ä¸€å¸§
        import cv2
        cv2.imwrite(str(output_dir / 'pose_sample.jpg'), pose_frames[0])

    # 3. è§†è§’æ£€æµ‹
    print("\n3ï¸âƒ£ è§†è§’æ£€æµ‹...")
    if view_mode == 'auto':
        view_detector = ViewAngleDetector()
        view_result = view_detector.detect_view_angle(keypoints_sequence)
        detected_view = view_result['primary_view']  # ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„é”®å
        view_confidence = view_result['confidence']
        print(f"   æ£€æµ‹è§†è§’: {get_view_name(detected_view)}")
        print(f"   ç½®ä¿¡åº¦: {view_confidence*100:.1f}%")
        print(f"   åˆ†æç­–ç•¥: {get_strategy_description(detected_view)}")
    else:
        detected_view = view_mode
        view_confidence = 1.0
        print(f"   ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šè§†è§’: {get_view_name(detected_view)}")

    # 4. è¿åŠ¨å­¦ç‰¹å¾è§£æï¼ˆä½¿ç”¨è‡ªé€‚åº”åˆ†æå™¨ï¼‰
    print("\n4ï¸âƒ£ è¿åŠ¨å­¦ç‰¹å¾è§£æ...")
    adaptive_analyzer = AdaptiveAnalyzer()
    kinematic_results = adaptive_analyzer.analyze(
        keypoints_sequence, fps,
        view_angle=detected_view
    )

    # åŸºç¡€æŒ‡æ ‡è¾“å‡º
    cadence_data = kinematic_results['cadence']
    print(f"   æ­¥é¢‘: {cadence_data['cadence']:.1f} æ­¥/åˆ†")
    print(f"   æ£€æµ‹æ­¥æ•°: {cadence_data['step_count']} æ­¥ (è§†é¢‘æ—¶é•¿ {cadence_data['duration']:.1f} ç§’)")
    if cadence_data.get('confidence', 0) > 0:
        print(f"   æ­¥é¢‘ç½®ä¿¡åº¦: {cadence_data['confidence']*100:.1f}%")

    # å‚ç›´æŒ¯å¹…ï¼ˆå½’ä¸€åŒ–ï¼‰
    vertical_motion = kinematic_results.get('vertical_motion', {})
    # ä¼˜å…ˆä½¿ç”¨å½’ä¸€åŒ–æŒ¯å¹…ï¼ˆamplitude_normalized æ˜¯ç›¸å¯¹èº¯å¹²é•¿åº¦çš„ç™¾åˆ†æ¯”ï¼‰
    if 'amplitude_normalized' in vertical_motion:
        amplitude_pct = vertical_motion['amplitude_normalized']
        print(f"   å‚ç›´æŒ¯å¹…: {amplitude_pct:.2f}% (èº¯å¹²é•¿åº¦)")
        rating = vertical_motion.get('amplitude_rating', {})
        if rating:
            print(f"   æŒ¯å¹…è¯„çº§: {rating.get('description', '')}")
    elif vertical_motion.get('amplitude', 0) > 0:
        print(f"   å‚ç›´æŒ¯å¹…: {vertical_motion['amplitude']:.4f} (å½’ä¸€åŒ–åæ ‡)")
    else:
        print(f"   å‚ç›´æŒ¯å¹…: æ•°æ®ä¸è¶³")

    # è†å…³èŠ‚è§’åº¦åˆ†æï¼ˆä¾§é¢è§†è§’ï¼‰
    if detected_view in ['side', 'mixed']:
        angles = kinematic_results.get('angles', {})
        knee_angles = angles.get('knee', {})
        if 'phase_analysis' in knee_angles:
            print("   è†å…³èŠ‚è§’åº¦ï¼ˆåˆ†é˜¶æ®µï¼‰:")
            phase_analysis = knee_angles['phase_analysis']
            for phase_name, phase_data in phase_analysis.items():
                phase_cn = {'ground_contact': 'è§¦åœ°', 'flight': 'è…¾ç©º', 'transition': 'è¿‡æ¸¡'}.get(phase_name, phase_name)
                if phase_data.get('count', 0) > 0:
                    print(f"      {phase_cn}: {phase_data['mean']:.1f}Â° (èŒƒå›´: {phase_data['min']:.1f}Â°-{phase_data['max']:.1f}Â°)")

    # å¯è§†åŒ–è§’åº¦æ›²çº¿
    if visualize and 'angles' in kinematic_results:
        print("   ç”Ÿæˆè§’åº¦æ›²çº¿å›¾...")
        try:
            plot_angle_curves(kinematic_results['angles'],
                              str(output_dir / 'angle_curves.png'))
        except Exception as e:
            print(f"   è­¦å‘Š: æ— æ³•ç”Ÿæˆè§’åº¦æ›²çº¿å›¾ - {e}")

    # 5. æ—¶åºæ·±åº¦å­¦ä¹ åˆ†æ
    print("\n5ï¸âƒ£ æ—¶åºæ·±åº¦å­¦ä¹ åˆ†æï¼ˆLSTM/CNNï¼‰...")
    temporal_analyzer = TemporalModelAnalyzer()
    temporal_results = temporal_analyzer.analyze(keypoints_sequence)

    print(f"   AIè´¨é‡è¯„åˆ†: {temporal_results['quality_score']:.2f}")
    print(f"   AIç¨³å®šæ€§: {temporal_results['stability_score']:.2f}")

    phase_dist = temporal_results['phase_distribution']
    print(f"   é˜¶æ®µåˆ†å¸ƒ: è§¦åœ°{phase_dist['ground_contact'] * 100:.1f}% | "
          f"è…¾ç©º{phase_dist['flight'] * 100:.1f}% | "
          f"è¿‡æ¸¡{phase_dist['transition'] * 100:.1f}%")

    # 6. è·‘æ­¥æŠ€æœ¯è´¨é‡è¯„ä»·
    print("\n6ï¸âƒ£ è·‘æ­¥æŠ€æœ¯è´¨é‡è¯„ä»·...")
    quality_evaluator = QualityEvaluator()
    quality_results = quality_evaluator.evaluate(
        kinematic_results, temporal_results,
        view_angle=detected_view
    )

    print(f"   æ€»ä½“è¯„åˆ†: {quality_results['total_score']:.2f}/100")
    print(f"   è¯„çº§: {quality_results['rating']}")

    # 7. AIæ–‡æœ¬åˆ†æ
    print("\n7ï¸âƒ£ AIæ–‡æœ¬åˆ†æä¸æŠ¥å‘Šç”Ÿæˆ...")
    ai_analyzer = AIAnalyzer()
    results_for_ai = {
        'quality_evaluation': quality_results,
        'kinematic_analysis': kinematic_results,
        'temporal_analysis': temporal_results,
        'view_angle': detected_view
    }
    ai_text = ai_analyzer.generate_analysis_report(results_for_ai)

    # ä¿å­˜AIæŠ¥å‘Š
    with open(output_dir / 'ai_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write(ai_text)
    print(f"   AIæŠ¥å‘Šå·²ä¿å­˜: {output_dir / 'ai_analysis_report.txt'}")

    # æ•´åˆç»“æœ
    complete_results = {
        'video_info': video_info,
        'view_angle': detected_view,
        'view_confidence': view_confidence,
        'kinematic_analysis': kinematic_results,
        'temporal_analysis': temporal_results,
        'quality_evaluation': quality_results,
        'ai_analysis': ai_text
    }

    # æ¸…ç†èµ„æº
    processor.release()
    estimator.close()

    return complete_results


def get_view_name(view: str) -> str:
    """è·å–è§†è§’ä¸­æ–‡åç§°"""
    names = {
        'side': 'ä¾§é¢è§†è§’',
        'front': 'æ­£é¢è§†è§’',
        'back': 'èƒŒé¢è§†è§’',
        'mixed': 'æ··åˆè§†è§’'
    }
    return names.get(view, view)


def get_strategy_description(view: str) -> str:
    """è·å–åˆ†æç­–ç•¥æè¿°"""
    strategies = {
        'side': 'è†å…³èŠ‚è§’åº¦ + å‚ç›´æŒ¯å¹… + èº¯å¹²å‰å€¾',
        'front': 'èº«ä½“å¯¹ç§°æ€§ + é«‹éƒ¨ç¨³å®šæ€§ + è†å¤–ç¿»æ£€æµ‹',
        'back': 'èº«ä½“å¯¹ç§°æ€§ + é«‹éƒ¨ç¨³å®šæ€§ + è¶³è·Ÿå¤–ç¿»æ£€æµ‹',
        'mixed': 'ç»¼åˆåˆ†æï¼ˆä¾§é¢+æ­£é¢æŒ‡æ ‡ï¼‰'
    }
    return strategies.get(view, 'æ ‡å‡†åˆ†æ')


def print_results(results: dict):
    """æ‰“å°åˆ†æç»“æœ"""
    quality = results['quality_evaluation']
    view_angle = results.get('view_angle', 'unknown')

    print("\n" + "=" * 80)
    print("ğŸ“Š åˆ†æç»“æœæ±‡æ€»")
    print("=" * 80)

    print(f"\nğŸ“ è§†è§’ä¿¡æ¯")
    print(f"   æ£€æµ‹è§†è§’: {get_view_name(view_angle)}")
    print(f"   ç½®ä¿¡åº¦: {results.get('view_confidence', 0)*100:.1f}%")

    print(f"\nğŸ¯ æ€»ä½“è¯„ä»·")
    print(f"   æŠ€æœ¯è´¨é‡è¯„åˆ†: {quality['total_score']:.2f}/100")
    print(f"   è¯„çº§: {quality['rating']}")

    print(f"\nğŸ“ˆ å„ç»´åº¦å¾—åˆ†")
    dims = quality.get('dimension_scores', {})
    print(f"   ç¨³å®šæ€§: {dims.get('stability', 0):.2f}")
    print(f"   æ•ˆç‡: {dims.get('efficiency', 0):.2f}")
    print(f"   è·‘å§¿: {dims.get('form', 0):.2f}")
    print(f"   èŠ‚å¥: {dims.get('rhythm', 0):.2f}")

    if quality.get('strengths'):
        print(f"\nâœ… ä¼˜åŠ¿")
        for strength in quality['strengths']:
            print(f"   â€¢ {strength}")

    if quality.get('weaknesses'):
        print(f"\nâš ï¸  è–„å¼±é¡¹")
        for weakness in quality['weaknesses']:
            print(f"   â€¢ {weakness}")

    if quality.get('suggestions'):
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
        for suggestion in quality['suggestions']:
            print(f"   â€¢ {suggestion}")


if __name__ == '__main__':
    main()
