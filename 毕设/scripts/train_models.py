#!/usr/bin/env python
"""
æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒè„šæœ¬

æ”¯æŒè®­ç»ƒï¼š
1. Transformeré˜¶æ®µåˆ†ç±»æ¨¡å‹
2. å¤šå°ºåº¦TCNè´¨é‡è¯„ä¼°æ¨¡å‹
3. è”åˆæ¨¡å‹ï¼ˆé˜¶æ®µåˆ†ç±» + è´¨é‡è¯„ä¼°ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_models.py --model joint --epochs 50 --batch_size 32

é€‚ç”¨äºæ¯•ä¸šè®¾è®¡ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„è·‘æ­¥åŠ¨ä½œè§†é¢‘è§£æä¸æŠ€æœ¯è´¨é‡è¯„ä»·ç³»ç»Ÿ
"""

import sys
import argparse
from pathlib import Path
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import numpy as np
from tqdm import tqdm

from config.config import CHECKPOINT_DIR, OUTPUT_DIR
from models.dataset import RunningDataset, MixedViewDataset, create_dataloaders


class Trainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, model, model_name: str, device: str = 'cpu'):
        self.model = model.to(device)
        self.model_name = model_name
        self.device = device

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }

    def train(self,
              train_loader: DataLoader,
              val_loader: DataLoader,
              epochs: int = 50,
              lr: float = 1e-3,
              weight_decay: float = 1e-4,
              patience: int = 10,
              save_best: bool = True) -> dict:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            patience: æ—©åœè€å¿ƒå€¼
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹

        Returns:
            è®­ç»ƒå†å²
        """
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr * 0.01)

        # æ—©åœ
        best_val_loss = float('inf')
        patience_counter = 0

        print(f"\n{'='*70}")
        print(f"å¼€å§‹è®­ç»ƒ: {self.model_name}")
        print(f"{'='*70}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        print(f"å­¦ä¹ ç‡: {lr}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        print(f"{'='*70}\n")

        start_time = time.time()

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(train_loader, optimizer)

            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(val_loader)

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]

            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)

            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1:3d}/{epochs} | "
                  f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
                  f"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | "
                  f"LR: {current_lr:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                if save_best:
                    self._save_checkpoint(f'{self.model_name}_best.pth')
            else:
                patience_counter += 1

            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"\næ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„")
                break

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(f'{self.model_name}.pth')

        # ä¿å­˜è®­ç»ƒå†å²
        self._save_history()

        elapsed_time = time.time() - start_time
        print(f"\n{'='*70}")
        print(f"è®­ç»ƒå®Œæˆ!")
        print(f"æ€»ç”¨æ—¶: {elapsed_time/60:.2f} åˆ†é’Ÿ")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"{'='*70}")

        return self.history

    def _train_epoch(self, train_loader: DataLoader, optimizer) -> tuple:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch in tqdm(train_loader, desc="Training", leave=False):
            keypoints, phase_labels, quality_scores, view_ids = batch
            keypoints = keypoints.to(self.device)
            phase_labels = phase_labels.to(self.device)
            quality_scores = quality_scores.to(self.device)
            view_ids = view_ids.to(self.device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            loss, acc = self._compute_loss_and_acc(
                keypoints, phase_labels, quality_scores, view_ids
            )

            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()
            correct += acc * len(keypoints)
            total += len(keypoints)

        avg_loss = total_loss / len(train_loader)
        avg_acc = correct / total * 100

        return avg_loss, avg_acc

    def _validate_epoch(self, val_loader: DataLoader) -> tuple:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validating", leave=False):
                keypoints, phase_labels, quality_scores, view_ids = batch
                keypoints = keypoints.to(self.device)
                phase_labels = phase_labels.to(self.device)
                quality_scores = quality_scores.to(self.device)
                view_ids = view_ids.to(self.device)

                loss, acc = self._compute_loss_and_acc(
                    keypoints, phase_labels, quality_scores, view_ids
                )

                total_loss += loss.item()
                correct += acc * len(keypoints)
                total += len(keypoints)

        avg_loss = total_loss / len(val_loader)
        avg_acc = correct / total * 100

        return avg_loss, avg_acc

    def _compute_loss_and_acc(self, keypoints, phase_labels, quality_scores, view_ids):
        """è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ - éœ€è¦å­ç±»å®ç°"""
        raise NotImplementedError

    def _save_checkpoint(self, filename: str):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        checkpoint_path = CHECKPOINT_DIR / filename
        CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
        torch.save(self.model.state_dict(), checkpoint_path)
        print(f"  ğŸ’¾ ä¿å­˜æ¨¡å‹: {checkpoint_path}")

    def _save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = OUTPUT_DIR / f'{self.model_name}_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)


class TransformerTrainer(Trainer):
    """Transformeræ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, device: str = 'cpu'):
        from models.transformer_model import RunningPhaseTransformer

        model = RunningPhaseTransformer(
            d_model=128,
            num_heads=8,
            num_layers=4,
            dropout=0.1
        )

        super().__init__(model, 'transformer_phase_model', device)
        self.criterion = nn.CrossEntropyLoss()

    def _compute_loss_and_acc(self, keypoints, phase_labels, quality_scores, view_ids):
        """è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡"""
        logits = self.model(keypoints, view_ids)  # (batch, seq, 3)

        # é‡å¡‘ç”¨äºäº¤å‰ç†µ
        logits_flat = logits.reshape(-1, 3)
        labels_flat = phase_labels.reshape(-1)

        loss = self.criterion(logits_flat, labels_flat)

        # è®¡ç®—å‡†ç¡®ç‡
        preds = torch.argmax(logits, dim=-1)
        acc = (preds == phase_labels).float().mean().item()

        return loss, acc


class QualityModelTrainer(Trainer):
    """è´¨é‡è¯„ä¼°æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, device: str = 'cpu'):
        from models.quality_model import RunningQualityModel

        model = RunningQualityModel(
            hidden_dim=128,
            num_levels=4,
            dropout=0.2
        )

        super().__init__(model, 'quality_tcn_model', device)
        self.criterion = nn.MSELoss()

    def _compute_loss_and_acc(self, keypoints, phase_labels, quality_scores, view_ids):
        """è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡"""
        outputs = self.model(keypoints, view_ids)
        pred_scores = outputs['scores']

        loss = self.criterion(pred_scores, quality_scores)

        # è®¡ç®—"å‡†ç¡®ç‡"ï¼ˆé¢„æµ‹è¯¯å·®å°äº5åˆ†çš„æ¯”ä¾‹ï¼‰
        error = torch.abs(pred_scores - quality_scores)
        acc = (error < 5).float().mean().item()

        return loss, acc


class JointModelTrainer(Trainer):
    """è”åˆæ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, device: str = 'cpu'):
        from models.quality_model import JointPhaseQualityModel

        model = JointPhaseQualityModel(
            hidden_dim=128,
            num_levels=4,
            dropout=0.2
        )

        super().__init__(model, 'joint_model', device)
        self.phase_criterion = nn.CrossEntropyLoss()
        self.quality_criterion = nn.MSELoss()

        # æŸå¤±æƒé‡
        self.phase_weight = 1.0
        self.quality_weight = 0.5

    def _compute_loss_and_acc(self, keypoints, phase_labels, quality_scores, view_ids):
        """è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡"""
        outputs = self.model(keypoints, view_ids)

        # é˜¶æ®µåˆ†ç±»æŸå¤±
        phase_logits = outputs['phase_logits'].reshape(-1, 3)
        phase_labels_flat = phase_labels.reshape(-1)
        phase_loss = self.phase_criterion(phase_logits, phase_labels_flat)

        # è´¨é‡è¯„ä¼°æŸå¤±
        quality_loss = self.quality_criterion(outputs['quality_scores'], quality_scores)

        # æ€»æŸå¤±
        loss = self.phase_weight * phase_loss + self.quality_weight * quality_loss

        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä½¿ç”¨é˜¶æ®µåˆ†ç±»å‡†ç¡®ç‡ï¼‰
        preds = torch.argmax(outputs['phase_logits'], dim=-1)
        acc = (preds == phase_labels).float().mean().item()

        return loss, acc


def train_all_models(args):
    """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
    device = 'cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nåˆ›å»ºæ•°æ®é›†...")
    train_loader, val_loader = create_dataloaders(
        batch_size=args.batch_size,
        num_train=args.num_train,
        num_val=args.num_val,
        num_workers=args.num_workers
    )

    models_to_train = []

    if args.model in ['all', 'transformer']:
        models_to_train.append(('Transformeré˜¶æ®µåˆ†ç±»', TransformerTrainer(device)))

    if args.model in ['all', 'quality']:
        models_to_train.append(('TCNè´¨é‡è¯„ä¼°', QualityModelTrainer(device)))

    if args.model in ['all', 'joint']:
        models_to_train.append(('è”åˆæ¨¡å‹', JointModelTrainer(device)))

    # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
    results = {}
    for name, trainer in models_to_train:
        print(f"\n\n{'#'*70}")
        print(f"# è®­ç»ƒæ¨¡å‹: {name}")
        print(f"{'#'*70}")

        history = trainer.train(
            train_loader,
            val_loader,
            epochs=args.epochs,
            lr=args.lr,
            weight_decay=args.weight_decay,
            patience=args.patience
        )
        results[name] = history

    # æ‰“å°æ€»ç»“
    print(f"\n\n{'='*70}")
    print("è®­ç»ƒæ€»ç»“")
    print('='*70)
    for name, history in results.items():
        best_val_loss = min(history['val_loss'])
        best_val_acc = max(history['val_acc'])
        print(f"{name}:")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print('='*70)


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒè·‘æ­¥åˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹')

    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--model', type=str, default='joint',
                        choices=['transformer', 'quality', 'joint', 'all'],
                        help='è¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')

    # æ•°æ®å‚æ•°
    parser.add_argument('--num_train', type=int, default=2000, help='è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--num_val', type=int, default=500, help='éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--num_workers', type=int, default=0, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')

    # å…¶ä»–
    parser.add_argument('--cpu', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨CPU')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # å¼€å§‹è®­ç»ƒ
    train_all_models(args)


if __name__ == '__main__':
    main()
